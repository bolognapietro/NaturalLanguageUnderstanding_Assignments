model,optimizer,lr,weight tying,variational drop,final_ppl

## 1.4 --> weight_tying and NO drop ##
LSTM_RNN,SGD,5,True,False,109.24898538778821 # batch size 32-64 and emb/hid_size 600        <--
LSTM_RNN,SGD,5,True,False,119.37828166003028 # batch size 64-128 and emb/hid_size 600
LSTM_RNN,SGD,5,True,False,118.02525226801154 # batch size 64-128 and emb/hid_size 300

## 1.5 --> weight_tying and fixed ##
LSTM_RNN,SGD,5,True,False,105.59080809872717 # batch size 32-64 and emb/hid_size 600 and drop 0.1        <--
LSTM_RNN,SGD,5,True,False,110.32624542159994 # batch size 64-128 and emb/hid_size 600 and drop 0.1
LSTM_RNN,SGD,5,True,False,88.47074018153941 # batch size 32-64 and emb/hid_size 600 and drop 0.5

## 1.6 --> weight_tying and variational drop ##
LSTM_RNN,SGD,5,True,True,112.2399392033356 # batch size 32-64 and emb/hid_size 600 and var_drop 0.1
LSTM_RNN,SGD,5,True,True,88.28096466096416 # batch size 32-64 and emb/hid_size 600 and var_drop 0.5     <--

## 1.7 --> weight_tying and variational drop and AvSGD ##
LSTM_RNN,ASGD,5,True,True,91.05789846392058 # batch size 32-64 and emb/hid_size 600 and var_drop 0.5     <--
LSTM_RNN,SGD,5,True,True,104.71614923670325 # batch size 32-64 and emb/hid_size 600 and var_drop 0.1
LSTM_RNN,ASGD,5,True,True,88.94015654714093 # batch size 32-64 and emb/hid_size 600 and var_drop 0.65


